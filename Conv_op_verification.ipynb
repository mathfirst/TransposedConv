{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f4a6e2",
   "metadata": {},
   "source": [
    "# Verify the Pytorch Conv-related APIs and write transposed Conv by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714cbc3a",
   "metadata": {},
   "source": [
    "## Use the class nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba65e676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2632, -0.3305,  0.2244],\n",
      "          [ 0.0297, -0.0928, -0.1374],\n",
      "          [ 0.1159,  0.1170,  0.1737]]]])\n",
      "tensor([[[[ 2.7325e+00,  5.0864e-01,  7.5504e-01, -9.9844e-01],\n",
      "          [ 1.4132e+00, -2.0177e+00, -2.4825e-01,  1.1625e+00],\n",
      "          [-3.0812e-01,  4.5179e-01,  9.7272e-04,  1.2158e+00],\n",
      "          [-1.5355e+00, -2.6443e-01,  1.9800e+00,  1.8255e-01]]]])\n",
      "tensor([[[[-0.4373, -0.5404],\n",
      "          [ 0.3233,  0.9529]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "kernel_size = 3\n",
    "batch_size = 1\n",
    "bias = False\n",
    "input_size = [batch_size, in_channels, 4, 4]\n",
    "# using the class nn.Conv2d\n",
    "conv_layer = nn.Conv2d(in_channels, out_channels, bias=bias, kernel_size=kernel_size)\n",
    "input_feature_map = torch.randn(input_size)\n",
    "output_feature_map = conv_layer(input_feature_map)\n",
    "print(conv_layer.weight.data) # in_channels * out_channels * height * width\n",
    "print(input_feature_map) \n",
    "print(output_feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab1eaa",
   "metadata": {},
   "source": [
    "## Use the function F.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3fc7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.4373, -0.5404],\n",
      "          [ 0.3233,  0.9529]]]])\n"
     ]
    }
   ],
   "source": [
    "# using the function F.conv2d\n",
    "output_feature_map_f = F.conv2d(input_feature_map, conv_layer.weight.data)\n",
    "print(output_feature_map_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d93c0",
   "metadata": {},
   "source": [
    "## 1. Use for-loops and matrix multiplications to implement conv operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a11ec7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8371,  0.0981, -2.8122],\n",
      "        [ 0.5762, -0.6831, -2.5968],\n",
      "        [-0.4870, -2.5456, -0.7429]])\n",
      "tensor([[-0.8371,  0.0981, -2.8122],\n",
      "        [ 0.5762, -0.6831, -2.5968],\n",
      "        [-0.4870, -2.5456, -0.7429]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def matrix_multiplication_for_conv2d(input_feature_map, kernel, bias=0, stride=1, padding=0):\n",
    "    if padding>0:\n",
    "        input_feature_map = F.pad(input_feature_map, pad=(padding, padding, padding, padding))\n",
    "        \n",
    "    input_h, input_w = input_feature_map.shape[-2:]\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    output_h = math.floor((input_h-kernel_h)/stride) + 1\n",
    "    output_w = math.floor((input_w-kernel_w)/stride) + 1\n",
    "    output_feature_map = torch.zeros(output_h, output_w)\n",
    "    for i in range(0, input_h-kernel_h+1, stride):\n",
    "        for j in range(0, input_h-kernel_h+1, stride):\n",
    "            region = input_feature_map[i:i+kernel_h, j:j+kernel_w]\n",
    "            output_feature_map[int(i/stride),int(j/stride)] = torch.sum( region*kernel ) + bias\n",
    "            \n",
    "    return output_feature_map\n",
    "\n",
    "input_data = torch.randn(5, 5)\n",
    "kernel = torch.randn(3, 3)\n",
    "bias = torch.randn(1)\n",
    "\n",
    "matmul_output = matrix_multiplication_for_conv2d(input_data, kernel, bias=bias, padding=1, stride=2)\n",
    "print(matmul_output)\n",
    "# call Pytorch API\n",
    "pytorch_api_conv_output = F.conv2d(input_data.reshape(1,1,input_data.shape[0],input_data.shape[1]),\\\n",
    "                                kernel.reshape(1,1,kernel.shape[0],kernel.shape[1]),\\\n",
    "                                padding=1, bias=bias, stride=2)\n",
    "print(pytorch_api_conv_output.squeeze(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ab74c",
   "metadata": {},
   "source": [
    "## 2. store all the flattened regions in a matrix and perform a matrix multiplication with a vectorized kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac23f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8371,  0.0981, -2.8122],\n",
      "        [ 0.5762, -0.6831, -2.5968],\n",
      "        [-0.4870, -2.5456, -0.7429]])\n",
      "tensor([[-0.8371,  0.0981, -2.8122],\n",
      "        [ 0.5762, -0.6831, -2.5968],\n",
      "        [-0.4870, -2.5456, -0.7429]])\n"
     ]
    }
   ],
   "source": [
    "def matrix_multiplication_for_conv2d_flatten(input_feature_map, kernel, bias=0, stride=1, padding=0):\n",
    "    if padding>0:\n",
    "        input_feature_map = F.pad(input_feature_map, pad=(padding, padding, padding, padding))\n",
    "        \n",
    "    input_h, input_w = input_feature_map.shape[-2:]\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    output_h = math.floor((input_h-kernel_h)/stride) + 1\n",
    "    output_w = math.floor((input_w-kernel_w)/stride) + 1\n",
    "    output_feature_map = torch.zeros(output_h, output_w)\n",
    "    region_matrix = torch.zeros(output_feature_map.numel(), kernel.numel())\n",
    "    kernel_vector = kernel.reshape(-1,1) # or kernel.reshape( kernel.numel(), 1 )\n",
    "    row_index = 0\n",
    "    for i in range(0, input_h-kernel_h+1, stride):\n",
    "        for j in range(0, input_h-kernel_h+1, stride):\n",
    "            region = input_feature_map[i:i+kernel_h, j:j+kernel_w]\n",
    "            region_flatten = region.flatten()\n",
    "            region_matrix[row_index] = region_flatten\n",
    "            row_index += 1\n",
    "            \n",
    "    output_feature_map = region_matrix @ kernel_vector + bias\n",
    "    output_feature_map = torch.reshape(output_feature_map, (output_h, output_w))\n",
    "            \n",
    "    return output_feature_map\n",
    "\n",
    "output_feature_map_flatten = matrix_multiplication_for_conv2d_flatten(input_data, kernel, bias=bias, padding=1, stride=2)\n",
    "print(output_feature_map_flatten)\n",
    "pytorch_api_conv_output = F.conv2d(input_data.reshape(1,1,input_data.shape[0],input_data.shape[1]),\\\n",
    "                                kernel.reshape(1,1,kernel.shape[0],kernel.shape[1]),\\\n",
    "                                padding=1, bias=bias, stride=2).squeeze(0).squeeze(0)\n",
    "print(pytorch_api_conv_output)\n",
    "\n",
    "torch.allclose(output_feature_map_flatten, pytorch_api_conv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbe753",
   "metadata": {},
   "source": [
    "## 3. full step 2 by considering batchsize and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a26e68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matrix_multiplication_for_conv2d(input_feature_map, kernel, bias=0, stride=1, padding=0):\n",
    "    if padding>0:\n",
    "        input_feature_map = F.pad(input_feature_map, pad=(padding, padding, padding, padding))\n",
    "\n",
    "    batch_size, in_channels, input_h, input_w = input_feature_map.shape\n",
    "    out_channels, in_channels, kernel_h, kernel_w = kernel.shape\n",
    "    output_h = math.floor((input_h-kernel_h)/stride) + 1\n",
    "    output_w = math.floor((input_w-kernel_w)/stride) + 1\n",
    "    output_feature_map = torch.zeros(batch_size, out_channels, output_h, output_w)\n",
    "    for ind in range(batch_size):\n",
    "        for oc in range(out_channels):\n",
    "            for ic in range(in_channels):\n",
    "                for i in range(0, input_h-kernel_h+1, stride):\n",
    "                    for j in range(0, input_h-kernel_h+1, stride):\n",
    "                        region = input_feature_map[ind, ic, i:i+kernel_h, j:j+kernel_w]\n",
    "                        output_feature_map[ind, oc, int(i/stride),int(j/stride)] += torch.sum( region*kernel[oc, ic] )\n",
    "            output_feature_map[ind, oc] += bias[oc]\n",
    "            \n",
    "    return output_feature_map\n",
    "\n",
    "input_data = torch.randn(2, 2, 5, 5)\n",
    "kernel = torch.randn(3, 2, 3, 3)\n",
    "bias = torch.randn(3)\n",
    "pytorch_api_conv2d_output = F.conv2d(input_data, kernel, bias=bias, padding=1, stride=2)\n",
    "matmul_for_conv2d_output = matrix_multiplication_for_conv2d(input_data, kernel, bias=bias, padding=1, stride=2)\n",
    "torch.allclose(pytorch_api_conv2d_output, matmul_for_conv2d_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c3bce",
   "metadata": {},
   "source": [
    "## 4. construct a kernel matrix, and use it to do conv2d, finally get transposed conv2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f826db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4418, -6.2556,  4.1600, -1.6744])\n",
      "tensor([[[[-2.4418, -6.2556],\n",
      "          [ 4.1600, -1.6744]]]])\n"
     ]
    }
   ],
   "source": [
    "def get_kernel_matrix(kernel, input_size):\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    input_h, input_w = input_size\n",
    "    output_h = input_h - kernel_h + 1\n",
    "    output_w = input_w - kernel_w + 1\n",
    "    num_output_entries = output_h*output_w\n",
    "    num_kernel_entries = kernel.numel()\n",
    "    result = torch.zeros(num_output_entries, input_h*input_w) # number of output entries * number of input entries\n",
    "    count = 0\n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            #print(i, input_h-kernel_h-i, j, input_w-kernel_w-j)\n",
    "            result[count] = torch.flatten(F.pad(kernel, (j, input_w-kernel_w-j, i, input_h-kernel_h-i)))\n",
    "            count += 1\n",
    "            \n",
    "    return result\n",
    "    \n",
    "input_data = torch.randn(4,4)\n",
    "kernel = torch.randn(3,3)\n",
    "kernel_matrix = get_kernel_matrix(kernel, input_data.shape)\n",
    "# print(kernel_matrix.shape)\n",
    "# print(kernel_matrix)\n",
    "\n",
    "mm_conv_output = kernel_matrix @ input_data.flatten() # resultant size: number of output elements * 1\n",
    "# unsqueeze twice for the dimensions of batchsize and output channels\n",
    "pytorch_conv_output = F.conv2d(input_data.unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0)) \n",
    "print(mm_conv_output)\n",
    "print(pytorch_conv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bdc6ff",
   "metadata": {},
   "source": [
    "### Transposed convolution is used to do upsamling, which will resotre the shape of the original input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a05e2616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  1.8623,   5.8987,   2.6517,  -0.6089],\n",
      "          [ -4.1699,   0.0925,  10.9981,   3.3901],\n",
      "          [ -1.5203, -17.5114,  -6.6500,   3.6920],\n",
      "          [  5.4847,   2.8560,  -3.8608,   0.7337]]]])\n",
      "tensor([[[[  1.8623,   5.8987,   2.6516,  -0.6089],\n",
      "          [ -4.1699,   0.0925,  10.9981,   3.3901],\n",
      "          [ -1.5203, -17.5114,  -6.6500,   3.6920],\n",
      "          [  5.4847,   2.8560,  -3.8608,   0.7337]]]])\n"
     ]
    }
   ],
   "source": [
    "## 2*2 -> 4*4\n",
    "mm_transposed_conv_output = kernel_matrix.transpose(-1,-2) @ mm_conv_output # matrix multiplication\n",
    "pytorch_transposed_conv_output = F.conv_transpose2d(pytorch_conv_output, kernel.unsqueeze(0).unsqueeze(0)) # Pytorch API\n",
    "print(mm_transposed_conv_output.reshape(1,1,4,4))\n",
    "print(pytorch_transposed_conv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3269d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
